# Linear Regression

## Least Square Method

> 概要
> > 最小二乘法的矩阵表达和几何意义
> > 从概率角度来看，最小二乘法就是噪声为高斯分布的MLE
> > 正则化
> > > L1 $\rightarrow$ Lasso
> > > L2 $\rightarrow$ Ridge（岭回归）
***

1. 线性回归要干什么
&emsp;&emsp;线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为

$$
y = w'x+e(e\sim{N(0,\sigma^2)})
$$
***

2. 线性回归的矩阵表述及其几何意义
&emsp;&emsp;假设现在给定的数据集为：

$$
D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\},x_i\in\mathbb{R}^p,y_i\in\mathbb{R}
$$

&emsp;&emsp;&emsp;&emsp;我们定义：
$$
\begin{gathered}
    X=(x_1,x_2,\cdots,x_n)^T \\
    Y=(y_1,y_2,\cdots,y_n)^T \\
    f(W)=X\beta,\beta\in\mathbb{R}^p
\end{gathered}
$$

&emsp;&emsp;&emsp;&emsp;对于最小二乘法，类似于统计中的均方误差，我们定义：
$$
\begin{aligned}
    S(\beta)&=\parallel{X\beta-Y}\parallel^2_2 \\
        &=(X\beta-Y)^T(X\beta-Y) \\
        &=\beta^TX^TX\beta-2\beta^TX^TY-Y^TY
\end{aligned}
$$
&emsp;&emsp;&emsp;&emsp;对$\beta$进行估计，有 $\hat{\beta}=\argmin\limits_\beta{S(\beta)}$，类似于MLE：
$$
\begin{aligned}
    \dfrac{\partial{S(\beta)}}{\partial{\beta}}&=X^TX\beta+\beta^TX^TX-2X^TY \\
    &=2X^TX\beta-2X^TY \\
    &=0
\end{aligned}
$$
> $X^TX\beta=(\beta^TX^TX)^T\in\mathbb{R}$

&emsp;&emsp;&emsp;&emsp;因为$X^TX$可逆，所以$\beta=X^\dagger{Y}$
>其中$X^\dagger=(X^TX)^{-1}X^T$，读作$X$的伪逆
***

3. 几何解释
&emsp;&emsp;$X_{\cdot1},X_{\cdot2},\cdots,X_{\cdot{p}}$形成了一个$\mathbb{R}^n$空间的$p$维子空间，现在需要在这个子空间中，找到和$Y$最接近的一个向量。
&emsp;&emsp;不难发现这个向量就是$Y$在该子空间中的投影，我们可以把投影向量表示成$X\beta$，由投影的性质可知，

$$
\forall{1\leq{i}\leq{p}},Y-X\beta\perp{X_{\cdot{i}}}
$$
&emsp;&emsp;&emsp;&emsp;所以，
$$
\begin{gathered}
    & X_{\cdot1}^T\cdot(Y-X\beta)=0 \\
    & X_{\cdot2}^T\cdot(Y-X\beta)=0 \\
    & \cdots \\
    & X_{\cdot{p}}^T\cdot(Y-X\beta)=0 \\
    & \\
    \Rightarrow& {X}^T(Y-X\beta)=0 \\
    \Rightarrow& \beta=(X^TX)^{-1}X^TY
\end{gathered}
$$
***

4. 概率角度
&emsp;&emsp;我们假设$y=f(\beta)+\epsilon$，其中$\epsilon\in{N(0,\sigma^2)}$，为数据的噪声。
&emsp;&emsp;所以有

$$
y|x;\beta\in{N(x\beta,\sigma^2)}
$$
&emsp;&emsp;&emsp;&emsp;对$\beta$进行极大似然估计(MLE)
$$
\begin{aligned}
    \mathcal{L(\beta)}&=\ln(P(Y\mid{X};\beta)) \\
    &\overset{iid}{=}\ln{\prod_{i=1}^{n}P(y_i\mid{x_i;\beta})} \\
    &=\sum_{i=1}^{n}\ln{P(y_i\mid{x_i;\beta})} \\
    &\overset{Gauss}{=}\sum_{i=1}^{n}(\ln(\frac{1}{\sqrt{2\pi}\sigma})+\ln(\exp(-\frac{(y_i-x_i\beta)^2}{2\sigma^2}))) \\
    &=\sum_{i=1}^{n}(\ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{(y_i-x_i\beta)^2}{2\sigma^2}) \\
    \hat{\beta}&=\argmax_\beta\mathcal{L(\beta)} \\
    &=\argmax_\beta\sum_{i=1}^{n}(-\frac{(y_i-x_i\beta)^2}{2\sigma^2}) \\
    &=\argmin_\beta\sum_{i=1}^{n}(y_i-x_i\beta)^2 \\
    &=\argmin_\beta\parallel{X\beta-Y}\parallel^2_2
\end{aligned}
$$
&emsp;&emsp;&emsp;&emsp;得到了和之前一样的结论。
> $LSE\overset{\text{Noise}\sim{N}}{\Longleftrightarrow}MLE$
***

5. 正则化
> 引入正则化是为了解决$\hat{\beta}$中$X^TX$不可逆的问题。  
> 正常来讲，对于数据样本我们有$n\gg{p}$，但是当样本数量不足或维数特别大的时候会出现过拟合，为了解决过拟合我们有三种办法：
> > 加数据  
> > 特征选择/特征提取  
> > 正则化

正则化的框架是：
$$
    \argmin_\beta[L(\beta)+\lambda{P(\beta)}]
$$
其中，$L(\beta)$是损失，$P(\beta)$是惩罚，根据采取的范数的不同，正则化又分为：
$$
\begin{aligned}
    &\mathcal{l_1}: \text{Lasso},P(\beta)=\parallel\beta\parallel_1 \\
    &\mathcal{l_2}: \text{Ridge},P(\beta)=\parallel\beta\parallel_2^2=\beta^T\beta（\text{岭回归，又称权值衰减}）
\end{aligned}
$$
下面讨论岭回归（概率角度）：
$$
\begin{aligned}
    \mathcal{J(\beta)}&=L(\beta)+\lambda{P(\beta)} \\
    &=(X\beta-Y)^T(X\beta-Y)+\lambda\beta^T\beta \\
    &=\beta^T(X^TX+\lambda{I})\beta-2\beta^TX^TY-Y^TY \\
    \dfrac{\partial{\mathcal{J}(\beta)}}{\partial{\beta}}&=2(X^TX+\lambda{I})\beta-2X^TY \\
    &=0 \\
    \hat{\beta}&=\argmin_\beta\mathcal{J(\beta)} \\
    &=(X^TX+\lambda{I})^{-1}X^TY
\end{aligned}
$$
此时$X^TX+\lambda{I}$一定可逆，因为对于$\forall{\vec{z}}\neq\vec{0}$，总有
$$
\begin{aligned}
    z^T(X^TX+\lambda{I})z&=(Xz)^T(Xz)+\lambda{z^Tz} \\
    &\geq\lambda{z^Tz}>0
\end{aligned}
$$$X^TX+\lambda{I}$是正定矩阵，必然可逆。
贝叶斯角度：
$$
    \beta\sim{N(0,\sigma_0^2)}
$$
用最大后验估计$MAP$：
$$
\begin{aligned}
    \hat{\beta}&=\argmax_\beta(P(\beta\mid{y})) \\
    &=\argmax_\beta\dfrac{P(y\mid{\beta})\cdot{P(\beta)}}{P(y)} \\
    &=\argmax_\beta\prod_{i=1}^{n}P(y\mid{\beta})\cdot{P(\beta)} \\
    &=\argmax_\beta(\sum_{i=1}^{n}(\ln(\frac{1}{\sqrt{2\pi}\sigma})-\frac{(y_i-x_i\beta)^2}{2\sigma^2})+\ln(\frac{1}{\sqrt{2\pi}\sigma_0})-\frac{\beta^T\beta}{2\sigma_0^2}) \\
    &=\argmin_\beta(\sum_{i=1}^{n}(y_i-x_i\beta)^2+\frac{\sigma^2}{\sigma_0^2}\beta^T\beta)
\end{aligned}
$$
令$\lambda=\dfrac{\sigma^2}{\sigma_0^2}$，$\hat{\beta}$和概率角度得出的估计值完全一致。
> ${Regularized\;LSE}\xLeftrightarrow[Noise\sim{N}]{prior\sim{N}}MAP$

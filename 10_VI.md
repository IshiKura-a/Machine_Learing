# Variational Inference

## Classified VI

​	回顾公式
$$
\log{p(x)}=\int_zq(z)\cdot\log\dfrac{p(x,z)}{q(z)}dz-\int_zq(z)\log\dfrac{p(z|x)}{q(z)}dz
$$
​	我们以另一个角度来看右边的第一项，令
$$
\mathcal{L}(q)=\int_zq(z)\cdot\log\dfrac{p(x,z)}{q(z)}dz
$$
​	$\mathcal{L}$的输入是一个概率密度函数$q$，$q(z)$实际上是一个变分，我们要让$q$充分逼近后验$p(z|x)$的话，那就会使得第二项相对熵逼近于0，在等式左侧不变的情况下，则$\mathcal{L}(q)$要尽可能的大。因此，
$$
\hat{q}(z)=\text{argmax}_{q(z)}\mathcal{L}(q)
$$
​	因为z是隐变量和参数的组合，我们不妨假设$z$可以分为$M$个相互独立的块，即
$$
q(z)=\prod_{i=1}^Mq_i(z_i)
$$
​	代入至$\mathcal{L}(q)$中，有
$$
\begin{aligned}
	\mathcal{L}(q)&=\int_zq(z)\cdot\log{p(x,z)}dz-\int_zq(z)\cdot\log{q(z)}dz \\
	&=\int_zq(z)\cdot\log{p(x,z)}dz_1dz_2\cdots{dz_M}-\int_zq(z)\cdot\log{q(z)}dz \\
\end{aligned}
$$
​	假设我们现在先固定其他$q_i$，求解$q_j$，那么
$$
\begin{aligned}
	\mathcal{L}(q)&=\int_{z_j}q_j(z_j)\int_{z_{i\ne{j}}}\prod_{i\ne{j}}^Mq_i(z_i)\log{p(x,z)}dz_{i\ne{j}}dz_j-\int_zq(z)\cdot\log{q(z)}dz \\
	&=\int_{z_j}q_j(z_j)\cdot{E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}dz_j-\int_zq(z)\cdot\log{q(z)}dz \\
	&=\int_{z_j}q_j(z_j)\cdot{E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}dz_j-\int_z\prod_{i=1}^M{q_i(z_i)}\cdot\sum_{i=1}^M\log{q_i(z_i)}dz \\
	&=\int_{z_j}q_j(z_j)\cdot{E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}dz_j-\sum_{i=1}^M\int_z\prod_{k=1}^M{q_k(z_k)}\cdot\log{q_i(z_i)}dz \\
	&=\int_{z_j}q_j(z_j)\cdot{E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}dz_j-\sum_{i=1}^M\int_{z_i}q_i(z_i)\log{q_i(z_i)}dz_i\prod_{k\ne{j}}^M\int_{z_{k}}q_k(z_k)dz_k \\
	&=\int_{z_j}q_j(z_j)\cdot{E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}dz_j-\sum_{i=1}^M\int_{z_i}q_i(z_i)\log{q_i(z_i)}dz_i \\
	&=\int_{z_j}q_j(z_j)\cdot{E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}dz_j-\int_{z_j}q_j(z_j)\log{q_j(z_j)}dz_j+C \\
\end{aligned}
$$
​	令
$$
\log\hat{p}(x,z_j)={E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}
$$
​	所以，
$$
\begin{aligned}
	\mathcal{L}(q)&=\int_{z_j}q_j(z_j)\cdot{E_{\prod_{i\ne{j}}^Mq_i(z_i)}[\log{p(x,z)}]}dz_j-\int_{z_j}q_j(z_j)\log{q_j(z_j)}dz_j+C \\
	&=\int_{z_j}q_j(z_j)\cdot\log\dfrac{\hat{p}(x,z_j)}{{q_j(z_j)}}dz_j+C \\
	&=-\text{KL}(q_j(z_j)||\hat{p}(x,z_j))+C
\end{aligned}
$$
​	由于相对熵非负，所以
$$
\hat{q_j}(z_j)=\text{argmax}_{q(z)}\mathcal{L}(q)=\hat{p}(x,z_j)
$$

​	以上的计算在面对多个样本的时候，需要把每个样本的$\log{p(x)}$的值进行叠加，加入计算。并且，我们假定$\Theta$（参数模型）是一组不变的常数。在我们计算完$\hat{q_{j}}$时，需要将其加入下一步迭代中，即
$$
\begin{aligned}
	&\hat{q_1}(z_1)=\int_{z_2}\cdots\int_{z_M}q_2\cdots{q_M}[\log_\Theta{p(x^{(i)},z)}]dz_2\cdots{dz_M} \\
	&\hat{q_2}(z_2)=\int_{z_1}\int_{z_3}\cdots\int_{z_M}\hat{q_1}q_3\cdots{q_M}[\log_\Theta{p(x^{(i)},z)}]dz_1dz_3\cdots{dz_M} \\
	\vdots \\
	&\hat{q_M}(z_M)=\int_{z_1}\cdots\int_{z_{M-1}}\hat{q_1}\cdots\hat{q_{M-1}}[\log_\Theta{p(x^{(i)},z)}]dz_1\cdots{dz_{M-1}}
\end{aligned}
$$
​	这也是一种坐标上升法，当$\mathcal{L}(q)$不增时，迭代可以终止。

​	经典的VI存在两个缺点，第一是它基于平均场理论的假设，即公式$(4)$，这在比较复杂的模型中是不成立的；第二，在我们进行迭代的时候，需要求期望，算积分，但是某些情况这些积分依旧无法求解， 

## SGVI

​	 之前介绍的VI是以坐标上升法实现的，在之前的章节中，提到过可以使用梯度上升法来完成优化，下面介绍随机梯度上升变分推断（SGVI）。

​	假设$q$是以$\phi$为参数的概率分布，那么我们求得$\phi$就可以得到$q$. 计算梯度
$$
\begin{aligned}
	\nabla_\phi\mathcal{L}(\phi)&=\nabla_\phi{E}_{q_\phi}[\log{p(x^{(i)},z)}-\log{q_\phi}] \\
	&=\nabla_\phi\int_zq_\phi\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz \\
	&=\int_z\nabla_\phi\{{q}_\phi[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]\}dz \\
	&=\int_z\nabla_\phi{q}_\phi\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz-\int_z{q}_\phi\cdot\frac{1}{{q}_\phi}\nabla_\phi{q}_\phi{}dz \\
	&=\int_z\nabla_\phi{q}_\phi\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz-\nabla_\phi\int_zq_\phi{}dz \\
	&=\int_z\nabla_\phi{q}_\phi\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz-\nabla_\phi1 \\
	&=\int_z\nabla_\phi{q}_\phi\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz \\
\end{aligned}
$$
​	如果这个积分可以用期望表示，那么我们就可以用蒙特卡洛采样的方法来解决，因此需要凑出一个$q_\phi$出来。
$$
\begin{aligned}
	\nabla_\phi\mathcal{L}(\phi)&=\int_z\nabla_\phi{q}_\phi\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz \\
	&=\int_z{q}_\phi\nabla_\phi\log{q}_\phi(z)\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz \\
	&=E_{q_\phi}[\nabla_\phi\log{q}_\phi\cdot(\log{p_\theta(x^{(i)},z)}-\log{q_\phi})]
\end{aligned}
$$

​	当然这样的方法存在问题，考虑$q_\phi$，当采样的点接近0时，因为是$\log{q_\phi}$，所以$q_\phi$的轻微变化会引起整体的巨大变大，即整体的方差会很大，这就意味着我们需要尽可能多的样本，才能很好的近似。因此，直接利用MC采样是不可行的。利用重参数化技巧(Reparameterization Trick)，我们可以降低方差。

​	在我们求梯度的时候，因为期望的分布和$\phi$有关，所以求解需要展开，非常困难，但是如果分布不是$q_\phi$而是一个确定的和$\phi$无关分布$p(\epsilon)$，那么我们可以对每一项求梯度，即
$$
	\nabla_\phi{E_{p(\epsilon)}}f(\phi)={E_{p(\epsilon)}}\nabla_\phi{f}(\phi)
$$
​	这样就会简化我们的算法。基于这样的想法，我们假设$z$是关于某个随机变量$\epsilon\sim{p(\epsilon)}$的函数，即
$$
z=g_\phi(\epsilon,x^{(i)})
$$
​	这时候$z$的随机性转移到了$\epsilon$上，这时候有
$$
	|q_\phi(z|x^{(i)})dz|=|p(\epsilon)d\epsilon|
$$
​	因此，
$$
\begin{aligned}
	\nabla_\phi\mathcal{L}(\phi)&=\nabla_\phi\int_z{q}_\phi(z)\cdot[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]dz \\
	&=\nabla_\phi\int_z[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]\cdot{p(\epsilon)}d\epsilon \\
	&=\nabla_\phi{E_{p(\epsilon)}}[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}] \\
	&=E_{p(\epsilon)}\nabla_\phi[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}] \\
	&=E_{p(\epsilon)}\nabla_z[\log{p_\theta(x^{(i)},z)}-\log{q_\phi}]\cdot\nabla_\phi{z} \\
	&=E_{p(\epsilon)}\nabla_z[\log{p_\theta(x^{(i)},z)}-\log{q_\phi(z|x^{(i)})}]\cdot\nabla_\phi{g_\phi(\epsilon,x^{(i)})}
\end{aligned}
$$
​	对于多个样本而言，公式变为
$$
\left\{
\begin{aligned}
	&\nabla_\phi\mathcal{L}(\phi)\approx\frac{1}{L}\sum_{l=1}^LE_{p(\epsilon)}\nabla_z[\log{p_\theta(x^{(l)},z)}-\log{q_\phi(z|x^{(l)})}]\cdot\nabla_\phi{g_\phi(\epsilon^{(l)},x^{(l)})} \\
	&\phi^{(t+1)}\leftarrow\phi^{(t)}+\lambda^{(t)}\cdot\nabla_\phi\mathcal{L}(\phi)
\end{aligned}
\right.
$$
